{
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "name": "",
  "signature": "sha256:d285ead7c7f53cc394af56ea5bb8de422a3ace0a43bf4afc6497938b10861fd0"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "News Group Text Classifier Part-2"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This Python code is an continuation of the R file available at https://github.com/Rajiv2806/20-News-Groups-Classifier/blob/master/News_Groups_Classifier_Part-1.Rmd"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#In Case if you havent installed sklearn package\n",
      "#!pip install --user sklearn #pip install --upgrade pip"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 21
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Importing the CSV file that we got as an output from our R program"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pandas as pd\n",
      "News20 = pd.read_csv(\"D:/ISB/34-DataMining-2/Assignment/20News.csv\")\n",
      "News20.columns"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 22,
       "text": [
        "Index([u'Unnamed: 0', u'id', u'doc_collection', u'Topic_Names'], dtype='object')"
       ]
      }
     ],
     "prompt_number": 22
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\n",
      "\n",
      "Looking at the sample structure of Dataset"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "News20.head(2)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "html": [
        "<div style=\"max-width:1500px;overflow:auto;\">\n",
        "<table border=\"1\" class=\"dataframe\">\n",
        "  <thead>\n",
        "    <tr style=\"text-align: right;\">\n",
        "      <th></th>\n",
        "      <th>Unnamed: 0</th>\n",
        "      <th>id</th>\n",
        "      <th>doc_collection</th>\n",
        "      <th>Topic_Names</th>\n",
        "    </tr>\n",
        "  </thead>\n",
        "  <tbody>\n",
        "    <tr>\n",
        "      <th>0</th>\n",
        "      <td>1</td>\n",
        "      <td>1</td>\n",
        "      <td>xref cantaloupe srv cs cmu alt atheism alt ath...</td>\n",
        "      <td>alt.atheism</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>1</th>\n",
        "      <td>2</td>\n",
        "      <td>2</td>\n",
        "      <td>xref cantaloupe srv cs cmu alt atheism alt ath...</td>\n",
        "      <td>alt.atheism</td>\n",
        "    </tr>\n",
        "  </tbody>\n",
        "</table>\n",
        "</div>"
       ],
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 23,
       "text": [
        "   Unnamed: 0  id                                     doc_collection  \\\n",
        "0           1   1  xref cantaloupe srv cs cmu alt atheism alt ath...   \n",
        "1           2   2  xref cantaloupe srv cs cmu alt atheism alt ath...   \n",
        "\n",
        "   Topic_Names  \n",
        "0  alt.atheism  \n",
        "1  alt.atheism  "
       ]
      }
     ],
     "prompt_number": 23
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Splitting up the data into training and Test\n",
      "\n",
      "2/3rd as Training Data and 1/3rd as test data"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.model_selection import train_test_split\n",
      "News20_Train,News20_Test = train_test_split(News20,test_size=0.33, random_state=42)\n",
      "print \"Training Data:\",len(News20_Train),\"\\nTesting Data:\",len(News20_Test)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Training Data: 13397 \n",
        "Testing Data: 6600\n"
       ]
      }
     ],
     "prompt_number": 24
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Naive Bayes on Complete Corpus"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Splitting up the Text as X Variable and Topic names of Y variable for Training and Text Datasets of the complete corpus"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.feature_extraction.text import TfidfVectorizer\n",
      "vect = TfidfVectorizer()\n",
      "X_Train = vect.fit_transform(News20_Train.doc_collection)\n",
      "print X_Train.shape\n",
      "\n",
      "X_Test = vect.transform(News20_Test.doc_collection)\n",
      "print X_Test.shape"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "(13397, 93034)\n",
        "(6600, 93034)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 25
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Applying the naive Bayes with an Laplacian smoothiong value of 30 we get the below accuracies for the training and test datasets"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.naive_bayes import MultinomialNB\n",
      "clf = MultinomialNB(alpha=30).fit(X_Train,News20_Train.Topic_Names)\n",
      "\n",
      "predict_train = clf.predict(X_Train)\n",
      "Accuracy_train = np.mean(predict_train == News20_Train.Topic_Names)*100\n",
      "print \"Accuracy of Training set:\",Accuracy_train\n",
      "\n",
      "predict_test = clf.predict(X_Test)\n",
      "Accuracy_test = np.mean(predict_test == News20_Test.Topic_Names)*100\n",
      "print \"Accuracy of Test set:\",Accuracy_test"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Accuracy of Training set: 92.5057848772\n",
        "Accuracy of Test set:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 89.803030303\n"
       ]
      }
     ],
     "prompt_number": 26
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Naive Bayes on top 5,000 words "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Picking up only the Top 5000 frequent words appearing in the whole corpus and splitting up the data as X & Y variables of training and test data\n",
      "\n",
      "We can see that there are 13,397 observations in the training data and 6600 observation in the training data with 500) features."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "vect_5000 = TfidfVectorizer(max_features=5000)\n",
      "X_Train_5000 = vect_5000.fit_transform(News20_Train.doc_collection)\n",
      "print X_Train_5000.shape\n",
      "\n",
      "X_Test_5000 = vect_5000.transform(News20_Test.doc_collection)\n",
      "print X_Test_5000.shape"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "(13397, 5000)\n",
        "(6600, 5000)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 27
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The Model fit on the top 5000 frequent words and the Training and testing accuracy are calculated."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "clf5000 = MultinomialNB(alpha=30).fit(X_Train_5000,News20_Train.Topic_Names)\n",
      "predict_train5000 = clf5000.predict(X_Train_5000)\n",
      "Accuracy_train5000 = np.mean(predict_train5000 == News20_Train.Topic_Names)*100\n",
      "print \"Accuracy of Training set:\",Accuracy_train5000\n",
      "\n",
      "predict_test5000 = clf5000.predict(X_Test_5000)\n",
      "Accuracy_test5000 = np.mean(predict_test5000 == News20_Test.Topic_Names)*100\n",
      "print \"Accuracy of Test set:\",Accuracy_test5000"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Accuracy of Training set: 91.9907441965\n",
        "Accuracy of Test set: 90.6212121212\n"
       ]
      }
     ],
     "prompt_number": 28
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Naive Bayes on top 10,000 words "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Picking up only the Top 10000 frequent words appearing in the whole corpus and splitting up the data as X & Y variables of training and test data\n",
      "\n",
      "We can see that there are 13,397 observations in the training data and 6600 observation in the training data with 10000 features/words."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "vect_10000 = TfidfVectorizer(max_features=10000)\n",
      "X_Train_10000 = vect_10000.fit_transform(News20_Train.doc_collection)\n",
      "print X_Train_10000.shape\n",
      "\n",
      "X_Test_10000 = vect_10000.transform(News20_Test.doc_collection)\n",
      "print X_Test_10000.shape"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "(13397, 10000)\n",
        "(6600, 10000)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 29
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The Model fit on the top 10000 frequent words and the Training and testing accuracy are calculated."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "clf10000 = MultinomialNB(alpha=30).fit(X_Train_10000,News20_Train.Topic_Names)\n",
      "predict_train10000 = clf10000.predict(X_Train_10000)\n",
      "Accuracy_train10000 = np.mean(predict_train10000 == News20_Train.Topic_Names)*100\n",
      "print \"Accuracy of Training set:\",Accuracy_train10000\n",
      "\n",
      "predict_test10000 = clf10000.predict(X_Test_10000)\n",
      "Accuracy_test10000 = np.mean(predict_test10000 == News20_Test.Topic_Names)*100\n",
      "print \"Accuracy of Test set:\",Accuracy_test10000"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Accuracy of Training set: 92.4087482272\n",
        "Accuracy of Test set: 90.8484848485\n"
       ]
      }
     ],
     "prompt_number": 30
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
      "#                                        IGNORE BELOW CODE\n",
      "#XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
      "\n",
      "#from sklearn.pipeline import Pipeline\n",
      "#text_clf = Pipeline([('vect', CountVectorizer()),\n",
      "                      ('tfidf', TfidfTransformer()),\n",
      "                      ('clf', MultinomialNB()),])\n",
      "#text_clf.fit(News20.doc_collection,News20.Topic_Names)\n",
      "#import numpy as np\n",
      "#docs_test = News20.doc_collection\n",
      "#predicted = text_clf.predict(docs_test)\n",
      "#np.mean(predicted == News20.Topic_Names)\n",
      "#from sklearn.linear_model import SGDClassifier\n",
      "#text_clf = Pipeline([('vect', CountVectorizer()),\n",
      "#                     ('tfidf', TfidfTransformer()),\n",
      "#                     ('clf', SGDClassifier(loss='hinge', penalty='l2',alpha=1e-3, random_state=42,\n",
      "#                                            #max_iter=5, tol=None\n",
      "#                                          )),])\n",
      "#text_clf.fit(News20.doc_collection, News20.Topic_Names)\n",
      "#predicted = text_clf.predict(docs_test)\n",
      "#np.mean(predicted == News20.Topic_Names)\n",
      "#from sklearn import metrics\n",
      "##print(metrics.classification_report(News20.id, predicted,target_names=News20.Topic_Names))\n",
      "##help(metrics.classification_report)\n",
      "#len(twenty.filenames)\n",
      "#from sklearn.datasets import fetch_20newsgroups\n",
      "#twenty_train = fetch_20newsgroups(subset='train', shuffle=True, random_state=42)\n",
      "#print(\"No. of classes:\", len(twenty_train.target_names))\n",
      "#twenty_train.target_names\n",
      "#len(twenty_train.data)\n",
      "#len(twenty_train.filenames)\n",
      "#print(\"\\n\".join(twenty_train.data[0].split(\"\\n\")[:3]))\n",
      "#print(twenty_train.target_names[twenty_train.target[3]])\n",
      "#twenty = fetch_20newsgroups(subset = 'all')\n",
      "#len(twenty.data)\n",
      "#print X_train_counts\n",
      "#print X_train_counts.shape\n",
      "#print Y_train_counts.shape\n",
      "#print X_test_counts.shape\n",
      "#print Y_test_counts.shape\n",
      "#X_train_counts\n",
      "#print len(News20.Topic_Names)\n",
      "#print News20.Topic_Names.unique()\n",
      "#print len(News20.Topic_Names.unique())\n",
      "#print len(News20_Train.Topic_Names)\n",
      "#print News20_Train.Topic_Names.unique()\n",
      "#print len(News20_Train.Topic_Names.unique())\n",
      "#print len(News20_Test.Topic_Names)\n",
      "#print News20_Test.Topic_Names.unique()\n",
      "#print len(News20_Test.Topic_Names.unique())\n",
      "#import numpy as np\n",
      "#import pandas as pd\n",
      "#from nltk import sent_tokenize, word_tokenize, pos_tag\n",
      "#t = word_tokenize(News20.doc_collection[0]) #t\n",
      "#X_train = count_vect.fit_transform(News20_Train.doc_collection)\n",
      "#Y_train = count_vect.fit_transform(News20_Train.Topic_Names)\n",
      "#X_test = count_vect.fit_transform(News20_Test.doc_collection)\n",
      "#Y_test = count_vect.fit_transform(News20_Test.Topic_Names)\n",
      "#News20.doc_collection[0]\n",
      "#print(\"\\n\".join(News20.doc_collection[0].split(\"\\n\")[:1]))\n",
      "#print(News20.Topic_Names[:4])\n",
      "#from sklearn.feature_extraction.text import CountVectorizer\n",
      "#count_vect = CountVectorizer()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "IndentationError",
       "evalue": "unexpected indent (<ipython-input-31-51df2031051d>, line 7)",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-31-51df2031051d>\"\u001b[1;36m, line \u001b[1;32m7\u001b[0m\n\u001b[1;33m    ('tfidf', TfidfTransformer()),\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m unexpected indent\n"
       ]
      }
     ],
     "prompt_number": 31
    }
   ],
   "metadata": {}
  }
 ]
}